{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I am first going to lemmatize my data, and then tokenize it by using two methods. One will be term frequency inverse docuemtn frequency transformer, and the other method will be cout vectorizing. I will use the count vectorized dataframe mainly for eda, as it simply counts the appearance of a word in a document (ourd document's will be rap songs) and thus is easily interpreted. I will reserve my TFIDF data for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in my recently cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./clean_data_years.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>hot100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My servants began to forge what was to become...</td>\n",
       "      <td>1999-04-20</td>\n",
       "      <td>?</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Things take a turn for the worse\" \"Send him b...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Absolutely</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One more beer And I'll take you all All of yo...</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>All Outta Ale</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yea, that's right It's not a Hardy Boy myster...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Angelz</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was back in the days when I met a brillian...</td>\n",
       "      <td>1999-04-20</td>\n",
       "      <td>Back in the Days</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HMMMM The flow is toe in, precision as an afr...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Ballskin</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you're waiting for a parade there ain't no...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Batty Boyz</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics        date  \\\n",
       "0   My servants began to forge what was to become...  1999-04-20   \n",
       "1  \"Things take a turn for the worse\" \"Send him b...  2009-03-24   \n",
       "2   One more beer And I'll take you all All of yo...  2002-01-01   \n",
       "3   Yea, that's right It's not a Hardy Boy myster...  2009-03-24   \n",
       "4   It was back in the days when I met a brillian...  1999-04-20   \n",
       "5   HMMMM The flow is toe in, precision as an afr...  2009-03-24   \n",
       "6   If you're waiting for a parade there ain't no...  2009-03-24   \n",
       "\n",
       "              title   artist    year  hot100  \n",
       "0                 ?  MF Doom  1999.0     0.0  \n",
       "1        Absolutely  MF Doom  2009.0     0.0  \n",
       "2     All Outta Ale  MF Doom  2002.0     0.0  \n",
       "3            Angelz  MF Doom  2009.0     0.0  \n",
       "4  Back in the Days  MF Doom  1999.0     0.0  \n",
       "5          Ballskin  MF Doom  2009.0     0.0  \n",
       "6        Batty Boyz  MF Doom  2009.0     0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating my Word Net Lemmatizer, which I'll use for lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that will lowercase my lyrics,get rid of quoutes and backslashs, as well remove 'ing' and its variants. This will ensure the lemmatization goes smoothly and works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "        df['lyrics'] = df['lyrics'].str.lower()\n",
    "        df['lyrics'] = df['lyrics'].str.replace('\"', '')\n",
    "        df['lyrics'] = df['lyrics'].str.replace('(,|\\.)', '')\n",
    "        df['lyrics'] = df['lyrics'].map(lambda x: re.sub(\"(in|ing|in')\", '', x))\n",
    "        \n",
    "        df['lyrics'] = df['lyrics'].map(lambda x: ' '.join([wnt.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking my work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>hot100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my servant began to forge what wa to become th...</td>\n",
       "      <td>1999-04-20</td>\n",
       "      <td>?</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thgs take a turn for the worse send him back w...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Absolutely</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one more beer and i'll take you all all of you...</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>All Outta Ale</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yea that's right it's not a hardy boy mystery ...</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>Angelz</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it wa back the day when i met a brilliant stud...</td>\n",
       "      <td>1999-04-20</td>\n",
       "      <td>Back in the Days</td>\n",
       "      <td>MF Doom</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics        date  \\\n",
       "0  my servant began to forge what wa to become th...  1999-04-20   \n",
       "1  thgs take a turn for the worse send him back w...  2009-03-24   \n",
       "2  one more beer and i'll take you all all of you...  2002-01-01   \n",
       "3  yea that's right it's not a hardy boy mystery ...  2009-03-24   \n",
       "4  it wa back the day when i met a brilliant stud...  1999-04-20   \n",
       "\n",
       "              title   artist    year  hot100  \n",
       "0                 ?  MF Doom  1999.0     0.0  \n",
       "1        Absolutely  MF Doom  2009.0     0.0  \n",
       "2     All Outta Ale  MF Doom  2002.0     0.0  \n",
       "3            Angelz  MF Doom  2009.0     0.0  \n",
       "4  Back in the Days  MF Doom  1999.0     0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving my lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./final_clean_lemma_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom list of stop words to not include in my tokenized data frames. A lot of things I'm excluding are things parts of words with punctuation, like the 'll' in 'I'll'. This is done because tokenizers tokenize on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = list(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords.extend(['like', 'll', 'ain','don','em','er','wa',\n",
    "                         'ya','just','let','got','den','ol','izz','im',\n",
    "                         'letting','hol','right','hah','dat','ve','mon',\n",
    "                         'la', 'aw','whit','ma','da','uhh','gon','wit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing my dataframe. I'm excluding words that don't appear in at least $5$ documents and excluding words that appear in over $70\\%$ of the documents. This will hopefully tame the size of my tokenized dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=custom_stopwords, min_df=5,max_df=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words=custom_stopwords, min_df=5,max_df=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = cvec.fit_transform(df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = tvec.fit_transform(df['lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tokenized dataframes out of my tokenized arrays to be used for modeling and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = pd.DataFrame(Xt.toarray(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token['date_year'] = df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(Xc.toarray(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['date_year'] = df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving my dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('./X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token.to_csv('./df_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17228, 22950)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
